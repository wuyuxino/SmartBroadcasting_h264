import os
import torch
import numpy as np
import queue
import asyncio
import threading
from datetime import datetime
from typing import List, Dict, Optional, Any, Tuple
from sklearn.preprocessing import StandardScaler
from fastapi import FastAPI, HTTPException, Request, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from pydantic import BaseModel, Field
import uvicorn
import logging
import logging.handlers
from contextlib import asynccontextmanager
from concurrent.futures import ThreadPoolExecutor
from threading import Lock
from collections import defaultdict
import time
import types

# å¯¼å…¥ä½ çš„æ¨¡å‹ç±»
from model import TrajectoryPredictor, KalmanFilter  
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded
from slowapi.middleware import SlowAPIMiddleware

# ===================== åŠ¨æ€è·¯å¾„åˆå§‹åŒ–ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼‰ =====================
# 1. è·å–å½“å‰è„šæœ¬ï¼ˆpredict_service.pyï¼‰çš„æ‰€åœ¨ç›®å½•ï¼ˆç»å¯¹è·¯å¾„ï¼‰
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))  # service/predict/
# 2. è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆSmartBroadcasting/ï¼Œå‘ä¸Šä¸¤çº§ï¼‰
PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, "../../"))  # ä»service/predict/å‘ä¸Šåˆ°SmartBroadcasting/


# ===================== å…¨å±€é…ç½®ï¼ˆWindows é«˜å¹¶å‘å…¼å®¹ç‰ˆï¼‰ =====================
CONFIG = {
    "model_path": os.path.join(PROJECT_ROOT, "service", "predict", "train_results", "best_model.pth"),
    "log_save_dir": os.path.join(PROJECT_ROOT, "service", "predict", "future_3frames_predictions"),
    "img_w": 3744,
    "img_h": 1920,
    # é«˜å¹¶å‘é…ç½®ï¼ˆWindows å…¼å®¹ï¼‰
    "max_workers": 8,  # çº¿ç¨‹æ± å¤§å°ï¼ˆCPUæ ¸å¿ƒæ•°*2ï¼‰
    "kf_pool_size": 32,  # å¡å°”æ›¼æ»¤æ³¢å™¨æ± å¤§å°
    "batch_size": 4,  # æ¨ç†æ‰¹å¤„ç†å¤§å°
    "timeout": 1.0,  # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    "max_concurrent_gpu": 4,  # æœ€å¤§å¹¶å‘GPUè¯·æ±‚æ•°
    "rate_limit": "10000/second",  # é™æµè°ƒæ•´ä¸º20QPS
}

# ===================== å…¨å±€å˜é‡ï¼ˆé«˜å¹¶å‘å®‰å…¨ï¼‰ =====================
# å…³é”®ï¼šç¡®ä¿DEVICEæ˜¯torch.deviceå¯¹è±¡ï¼Œä¸”ä¸è¢«è¦†ç›–
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MODEL = None
SCALER = None
TARGET_SCALER = None
MODEL_CONFIG = None
KF_POOL = []  # æ”¹ä¸ºæ± åŒ–åˆ—è¡¨
KF_POOL_LOCK = Lock()  # æ»¤æ³¢å™¨æ± é”
LOG_LISTENER = None
EXECUTOR = None  # å…¨å±€çº¿ç¨‹æ± 
GPU_SEMAPHORE = None  # GPUå¹¶å‘æ§åˆ¶ä¿¡å·é‡
REQUEST_METRICS = defaultdict(int)  # è¯·æ±‚ç»Ÿè®¡
METRICS_LOCK = Lock()

# ===================== å·¥å…·å‡½æ•°ï¼ˆæ–°å¢å‚æ•°æ ¡éªŒï¼‰ =====================
def validate_model_config(config: Dict) -> Dict:
    """æ ¡éªŒå¹¶æ¸…ç†æ¨¡å‹é…ç½®ï¼ˆå¼ºåˆ¶æ‰€æœ‰å‚æ•°ä¸ºæ•´æ•°ï¼Œé˜²æ­¢Conv1då‚æ•°é”™è¯¯ï¼‰"""
    required_keys = ['input_dim', 'pred_len', 'd_model', 'nhead', 'num_layers', 'conv_channels']
    clean_config = {}
    
    for key in required_keys:
        if key not in config:
            raise ValueError(f"æ¨¡å‹é…ç½®ç¼ºå°‘å…³é”®é”®ï¼š{key}")
        
        val = config[key]
        # è¿‡æ»¤å‡½æ•°å¯¹è±¡
        if isinstance(val, (types.FunctionType, types.MethodType)):
            raise TypeError(f"æ¨¡å‹é…ç½®ä¸­ {key} æ˜¯å‡½æ•°å¯¹è±¡ï¼ˆé¢„æœŸæ•°å€¼ï¼‰")
        # å…³é”®ä¿®æ­£ï¼šæ‰€æœ‰æ¨¡å‹å‚æ•°å¼ºåˆ¶è½¬ä¸ºæ•´æ•°ï¼ˆConv1dè¦æ±‚æ•´æ•°ï¼‰
        try:
            clean_config[key] = int(val)  # åŒ…æ‹¬d_modelåœ¨å†…ï¼Œå…¨éƒ¨è½¬æ•´æ•°
        except (ValueError, TypeError) as e:
            raise TypeError(f"æ¨¡å‹é…ç½® {key} æ— æ³•è½¬æ¢ä¸ºæ•´æ•°ï¼š{val}ï¼ˆç±»å‹ï¼š{type(val)}ï¼‰â†’ é”™è¯¯ï¼š{e}")
    
    # è¡¥å……é»˜è®¤å€¼
    clean_config.setdefault('dropout', 0.0)
    return clean_config

def numpy_to_python(obj):
    """é€’å½’è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹"""
    if isinstance(obj, (np.integer, np.floating)):
        return float(obj) if isinstance(obj, np.floating) else int(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {k: numpy_to_python(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [numpy_to_python(i) for i in obj]
    else:
        return obj

def update_metrics(key: str):
    """çº¿ç¨‹å®‰å…¨æ›´æ–°è¯·æ±‚æŒ‡æ ‡"""
    with METRICS_LOCK:
        REQUEST_METRICS[key] += 1

class AsyncRotatingFileHandler(logging.handlers.RotatingFileHandler):
    """å¼‚æ­¥æ—¥å¿—å¤„ç†å™¨"""
    def emit(self, record):
        try:
            asyncio.get_event_loop().call_soon_threadsafe(super().emit, record)
        except Exception:
            self.handleError(record)

# ===================== å¼‚æ­¥æ—¥å¿—åˆå§‹åŒ–ï¼ˆé«˜å¹¶å‘ç‰ˆï¼‰ =====================
def init_async_logger():
    """é«˜å¹¶å‘å¼‚æ­¥æ—¥å¿—åˆå§‹åŒ–"""
    log_dir = CONFIG["log_save_dir"]
    os.makedirs(log_dir, exist_ok=True)
    log_path = os.path.join(log_dir, "api_high_concurrency.log")

    # 1. åˆ›å»ºå¼‚æ­¥æ—¥å¿—å¤„ç†å™¨
    file_handler = AsyncRotatingFileHandler(
        log_path, maxBytes=50*1024*1024, backupCount=10, encoding="utf-8"
    )
    file_handler.setFormatter(logging.Formatter(
        "%(asctime)s - %(threadName)s - %(levelname)s - %(message)s"
    ))

    # 2. é…ç½®æ ¹æ—¥å¿—
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    root_logger.handlers.clear()
    root_logger.addHandler(file_handler)
    root_logger.addHandler(logging.StreamHandler())

    return file_handler

# ===================== å¡å°”æ›¼æ»¤æ³¢å™¨æ± ç®¡ç†ï¼ˆé«˜å¹¶å‘ç‰ˆï¼‰ =====================
def init_kf_pool():
    """åˆå§‹åŒ–å¡å°”æ›¼æ»¤æ³¢å™¨æ± """
    global KF_POOL
    KF_POOL = [
        KalmanFilter(dt=1.0, std_acc=1.0, std_meas=0.1)
        for _ in range(CONFIG["kf_pool_size"])
    ]
    logging.info(f"âœ… å¡å°”æ›¼æ»¤æ³¢å™¨æ± åˆå§‹åŒ–å®Œæˆï¼Œå¤§å°ï¼š{CONFIG['kf_pool_size']}")

def get_kf_from_pool() -> KalmanFilter:
    """ä»æ± è·å–æ»¤æ³¢å™¨ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    with KF_POOL_LOCK:
        if KF_POOL:
            return KF_POOL.pop()
        # æ± ç©ºæ—¶åˆ›å»ºä¸´æ—¶æ»¤æ³¢å™¨
        return KalmanFilter(dt=1.0, std_acc=1.0, std_meas=0.1)

def return_kf_to_pool(kf: KalmanFilter):
    """å½’è¿˜æ»¤æ³¢å™¨åˆ°æ± ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    with KF_POOL_LOCK:
        if len(KF_POOL) < CONFIG["kf_pool_size"]:
            KF_POOL.append(kf)

# ===================== æ¨¡å‹é¢„åŠ è½½ï¼ˆæ–°å¢é”™è¯¯é˜²æŠ¤ï¼‰ =====================
def preload_resources():
    """é«˜å¹¶å‘æ¨¡å‹é¢„åŠ è½½ï¼ˆå«å‚æ•°æ ¡éªŒ+é”™è¯¯é˜²æŠ¤ï¼‰"""
    global MODEL, SCALER, TARGET_SCALER, MODEL_CONFIG, EXECUTOR, GPU_SEMAPHORE
    
    try:
        # 1. åˆå§‹åŒ–GPUå¹¶å‘æ§åˆ¶
        GPU_SEMAPHORE = asyncio.Semaphore(CONFIG["max_concurrent_gpu"]) if DEVICE.type == "cuda" else None
        
        # 2. åˆå§‹åŒ–çº¿ç¨‹æ± 
        EXECUTOR = ThreadPoolExecutor(
            max_workers=CONFIG["max_workers"],
            thread_name_prefix="infer_worker"
        )
        
        # 3. åŠ è½½å¹¶æ ¡éªŒcheckpoint
        logging.info(f"ğŸ“¥ åŠ è½½æ¨¡å‹ checkpointï¼š{CONFIG['model_path']}")
        checkpoint = torch.load(CONFIG["model_path"], map_location=DEVICE, weights_only=False)
        
        # å…³é”®ï¼šæ ¡éªŒæ¨¡å‹é…ç½®ï¼ˆå¼ºåˆ¶æ‰€æœ‰å‚æ•°ä¸ºæ•´æ•°ï¼‰
        if 'config' not in checkpoint:
            raise ValueError("Checkpoint ç¼ºå°‘ 'config' é”®")
        MODEL_CONFIG = validate_model_config(checkpoint['config'])
        logging.info(f"âœ… æ¨¡å‹é…ç½®æ ¡éªŒé€šè¿‡ï¼ˆå…¨æ•´æ•°ï¼‰ï¼š{MODEL_CONFIG}")
        
        # 4. åˆå§‹åŒ–æ ‡å‡†åŒ–å™¨ï¼ˆå¢åŠ ç±»å‹æ ¡éªŒï¼‰
        for scaler_key in ['scaler_mean', 'scaler_scale', 'target_scaler_mean', 'target_scaler_scale']:
            if scaler_key not in checkpoint:
                raise ValueError(f"Checkpoint ç¼ºå°‘ {scaler_key}")
            if not isinstance(checkpoint[scaler_key], (list, np.ndarray)):
                raise TypeError(f"{scaler_key} ä¸æ˜¯åˆ—è¡¨/æ•°ç»„ï¼š{type(checkpoint[scaler_key])}")

        SCALER = StandardScaler()
        SCALER.mean_ = np.array(checkpoint['scaler_mean'], dtype=np.float32)
        SCALER.scale_ = np.array(checkpoint['scaler_scale'], dtype=np.float32)
        SCALER.scale_[SCALER.scale_ < 1e-6] = 1e-6

        TARGET_SCALER = StandardScaler()
        TARGET_SCALER.mean_ = np.array(checkpoint['target_scaler_mean'], dtype=np.float32)
        TARGET_SCALER.scale_ = np.array(checkpoint['target_scaler_scale'], dtype=np.float32)
        TARGET_SCALER.scale_[TARGET_SCALER.scale_ < 1e-6] = 1e-6

        # 5. åˆå§‹åŒ–Transformeræ¨¡å‹ï¼ˆå…³é”®ï¼šç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½æ˜¯æ•´æ•°ï¼‰
        logging.info(f"ğŸ”§ åˆå§‹åŒ–æ¨¡å‹ï¼Œè®¾å¤‡ï¼š{DEVICE}ï¼Œå‚æ•°å…¨æ•´æ•°")
        # æ˜¾å¼æå–æ•´æ•°å‚æ•°ï¼Œé¿å…æµ®ç‚¹æ•°ä¼ å…¥
        input_dim = int(MODEL_CONFIG['input_dim'])
        pred_len = int(MODEL_CONFIG['pred_len'])
        d_model = int(MODEL_CONFIG['d_model'])
        nhead = int(MODEL_CONFIG['nhead'])
        num_layers = int(MODEL_CONFIG['num_layers'])
        conv_channels = int(MODEL_CONFIG['conv_channels'])
        
        model = TrajectoryPredictor(
            input_dim=input_dim,
            output_dim=pred_len * 2,  # æ˜¾å¼è®¡ç®—ï¼Œç¡®ä¿æ•´æ•°
            d_model=d_model,
            nhead=nhead,
            num_layers=num_layers,
            conv_channels=conv_channels,
            dropout=0.0  # æ¨ç†æ—¶ç¦ç”¨dropout
        )
        # å…ˆåŠ è½½æƒé‡ï¼Œå†ç§»åˆ°è®¾å¤‡ï¼ˆé¿å…è®¾å¤‡ä¸åŒ¹é…ï¼‰
        model.load_state_dict(checkpoint['model_state_dict'], strict=True)
        MODEL = model.to(DEVICE)  # å…³é”®ï¼šç¡®ä¿modelæ˜¯å®ä¾‹ï¼Œè€Œéå‡½æ•°
        MODEL.eval()  # çº¯evalæ¨¡å¼
        
        # 6. GPUä¼˜åŒ–ï¼ˆç¡®è®¤DEVICEæ˜¯torch.deviceå¯¹è±¡ï¼‰
        if isinstance(DEVICE, torch.device) and DEVICE.type == "cuda":
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            torch.set_grad_enabled(False)
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        else:
            logging.warning(f"âš ï¸ DEVICE ä¸æ˜¯CUDAè®¾å¤‡ï¼š{DEVICE}ï¼ˆç±»å‹ï¼š{type(DEVICE)}ï¼‰")
        
        # 7. æ¨¡å‹é¢„çƒ­ï¼ˆç¡®ä¿è¾“å…¥å°ºå¯¸æ˜¯æ•´æ•°ï¼‰
        dummy_input = torch.randn(1, 10, len(SCALER.mean_), dtype=torch.float32).to(DEVICE)
        with torch.no_grad():
            for _ in range(5):
                MODEL(dummy_input)
            if DEVICE.type == "cuda":
                torch.cuda.synchronize()
        
        # 8. åˆå§‹åŒ–å¡å°”æ›¼æ»¤æ³¢å™¨æ± 
        init_kf_pool()
        
        logging.info(f"""âœ… é«˜å¹¶å‘æ¨¡å‹é¢„åŠ è½½æˆåŠŸï¼
        - è®¾å¤‡ç±»å‹ï¼š{type(DEVICE)} â†’ {DEVICE}
        - æ¨¡å‹é…ç½®ï¼ˆå…¨æ•´æ•°ï¼‰ï¼š{MODEL_CONFIG}
        - çº¿ç¨‹æ± å¤§å°ï¼š{CONFIG['max_workers']}
        - GPUæœ€å¤§å¹¶å‘ï¼š{CONFIG['max_concurrent_gpu']}
        """)
        
    except Exception as e:
        logging.error(f"âŒ æ¨¡å‹é¢„åŠ è½½å¤±è´¥ï¼š{str(e)}", exc_info=True)
        # æ‰“å°å…³é”®å˜é‡ç±»å‹ï¼Œè¾…åŠ©å®šä½
        logging.error(f"ğŸ” å…³é”®å˜é‡ç±»å‹æ’æŸ¥ï¼š")
        logging.error(f"   - DEVICE: {type(DEVICE)} â†’ {DEVICE}")
        logging.error(f"   - MODEL_CONFIG: {type(MODEL_CONFIG)} â†’ {MODEL_CONFIG if MODEL_CONFIG else 'æœªåŠ è½½'}")
        logging.error(f"   - checkpoint['config']: {type(checkpoint.get('config')) if 'checkpoint' in locals() else 'æœªåŠ è½½'}")
        # æ‰“å°Conv1dç›¸å…³å‚æ•°
        if 'MODEL_CONFIG' in locals() and MODEL_CONFIG:
            logging.error(f"   - d_modelï¼ˆæ•´æ•°æ ¡éªŒåï¼‰: {MODEL_CONFIG['d_model']}ï¼ˆç±»å‹ï¼š{type(MODEL_CONFIG['d_model'])}ï¼‰")
            logging.error(f"   - conv_channels: {MODEL_CONFIG['conv_channels']}ï¼ˆç±»å‹ï¼š{type(MODEL_CONFIG['conv_channels'])}ï¼‰")
        raise RuntimeError(f"æ¨¡å‹é¢„åŠ è½½å¤±è´¥ï¼š{str(e)}")

# ===================== FastAPIç”Ÿå‘½å‘¨æœŸç®¡ç† =====================
@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPIç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼ˆå¯åŠ¨/å…³é—­ï¼‰"""
    # å¯åŠ¨é˜¶æ®µ
    global LOG_LISTENER
    LOG_LISTENER = init_async_logger()
    try:
        preload_resources()
    except Exception as e:
        logging.critical(f"ğŸ’¥ æœåŠ¡å¯åŠ¨å¤±è´¥ï¼š{e}")
        raise  # ç»ˆæ­¢æœåŠ¡å¯åŠ¨
    logging.info("ğŸš€ é«˜å¹¶å‘è½¨è¿¹é¢„æµ‹APIå¯åŠ¨å®Œæˆï¼ˆWindowså…¼å®¹ç‰ˆï¼‰")
    yield
    # å…³é—­é˜¶æ®µ
    if EXECUTOR:
        EXECUTOR.shutdown(wait=True)
    if DEVICE.type == "cuda":
        torch.cuda.empty_cache()
    logging.info("ğŸ›‘ é«˜å¹¶å‘è½¨è¿¹é¢„æµ‹APIå·²å…³é—­")

# ===================== FastAPIå®ä¾‹åˆ›å»º =====================
app = FastAPI(
    title="è½¨è¿¹é¢„æµ‹APIï¼ˆé«˜å¹¶å‘ç‰ˆï¼‰",
    description="æ”¯æŒæ¯ç§’20æ¬¡è¯·æ±‚ï¼ŒWindowså…¼å®¹",
    version="3.0.0",
    lifespan=lifespan
)

# ===================== ä¸­é—´ä»¶é…ç½® =====================
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
app.add_middleware(GZipMiddleware, minimum_size=1000)
app.add_middleware(SlowAPIMiddleware)

# ===================== é™æµé…ç½® =====================
limiter = Limiter(
    key_func=get_remote_address,
    default_limits=[CONFIG["rate_limit"]],
    storage_uri="memory://"
)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# ===================== æ•°æ®æ¨¡å‹ =====================
class FrameData(BaseModel):
    frame_id: int = Field(..., description="å¸§IDï¼ˆå”¯ä¸€ã€é€’å¢ï¼‰")
    x1: float = Field(..., description="ç›®æ ‡æ¡†å·¦ä¸Šè§’xåæ ‡")
    y1: float = Field(..., description="ç›®æ ‡æ¡†å·¦ä¸Šè§’yåæ ‡")
    w: float = Field(..., description="ç›®æ ‡æ¡†å®½åº¦")
    h: float = Field(..., description="ç›®æ ‡æ¡†é«˜åº¦")
    conf: float = Field(..., ge=0.0, le=1.0, description="ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰")

class PredictRequest(BaseModel):
    frame_data: List[FrameData] = Field(..., description="å¸§æ•°æ®åˆ—è¡¨ï¼ˆéœ€â‰¥10å¸§æœ‰æ•ˆæ•°æ®ï¼‰")
    use_kalman: bool = Field(False, description="æ˜¯å¦å¯ç”¨å¡å°”æ›¼æ»¤æ³¢")
    conf_thresh: float = Field(0.5, ge=0.0, le=1.0, description="ç½®ä¿¡åº¦è¿‡æ»¤é˜ˆå€¼")

class PredictResponse(BaseModel):
    code: int = Field(..., description="çŠ¶æ€ç ï¼š200æˆåŠŸ/500å¤±è´¥")
    msg: str = Field(..., description="çŠ¶æ€ä¿¡æ¯")
    data: Optional[Dict[str, Any]] = Field(None, description="é¢„æµ‹ç»“æœï¼ˆJSONæ ¼å¼ï¼‰")
    error: Optional[str] = Field(None, description="é”™è¯¯è¯¦æƒ…")
    latency: Optional[float] = Field(None, description="è¯·æ±‚è€—æ—¶ï¼ˆç§’ï¼‰")

# ===================== æ ¸å¿ƒæ¨ç†å‡½æ•° =====================
def process_frame_data(frame_data: List[FrameData], conf_thresh: float) -> tuple[List[np.ndarray], List[int]]:
    """å¤„ç†å¸§æ•°æ®"""
    sorted_frames = sorted(frame_data, key=lambda x: x.frame_id)
    if len(sorted_frames) < 10:
        raise ValueError(f"å¸§æ•°æ®ä¸è¶³ï¼ˆéœ€â‰¥10å¸§ï¼‰ï¼Œå®é™…ä»…{len(sorted_frames)}å¸§")

    max_frames = len(sorted_frames)
    real_frames = np.zeros((max_frames, 4), dtype=np.float32)
    frame_ids = []
    valid_idx = 0

    for frame in sorted_frames:
        if frame.conf < conf_thresh:
            continue
        real_frames[valid_idx, 0] = frame.x1 + frame.w / 2
        real_frames[valid_idx, 1] = frame.y1 + frame.h / 2
        real_frames[valid_idx, 2] = frame.w
        real_frames[valid_idx, 3] = frame.h
        frame_ids.append(frame.frame_id)
        valid_idx += 1

    real_frames = real_frames[:valid_idx]
    if len(real_frames) < 10:
        raise ValueError(f"æœ‰æ•ˆå¸§ä¸è¶³ï¼ˆéœ€â‰¥10å¸§ï¼‰ï¼Œå®é™…ä»…{len(real_frames)}å¸§")
    
    real_frames_list = [real_frames[i] for i in range(len(real_frames))]
    return real_frames_list, frame_ids

def preprocess_history_batch(history_list: List[List[np.ndarray]]) -> torch.Tensor:
    """æ‰¹é‡é¢„å¤„ç†å†å²æ•°æ®"""
    batch_data = []
    for history_frames in history_list:
        history = history_frames[-10:] if len(history_frames) > 10 else history_frames
        if len(history) < 10:
            history = [history[0]] * (10 - len(history)) + history
        
        history_np = np.array(history, dtype=np.float32)
        delta = np.zeros_like(history_np)
        delta[1:] = history_np[1:] - history_np[:-1]
        input_feat = np.concatenate([history_np, delta], axis=1)
        batch_data.append(SCALER.transform(input_feat))
    
    batch_tensor = torch.FloatTensor(np.array(batch_data)).to(DEVICE)
    return batch_tensor

def predict_future_batch(
        history_list: List[List[np.ndarray]],
        kf_list: List[Optional[KalmanFilter]],
        current_frame_ids: List[int]
) -> Tuple[List[List[Dict]], List[List[int]], List[List[np.ndarray]]]:
    """æ‰¹é‡é¢„æµ‹æœªæ¥3å¸§"""
    # æ‰¹é‡é¢„å¤„ç†
    input_seq = preprocess_history_batch(history_list)
    
    # æ‰¹é‡æ¨ç†
    with torch.no_grad():
        pred_norm = MODEL(input_seq)
        if DEVICE.type == "cuda":
            torch.cuda.synchronize()
    
    # æ‰¹é‡åæ ‡å‡†åŒ–
    pred_raw_all = TARGET_SCALER.inverse_transform(
        pred_norm.cpu().numpy().reshape(-1, 6)
    ).reshape(-1, 3, 2)
    
    # æ‰¹é‡å¤„ç†ç»“æœ
    all_pred_results = []
    all_future_ids = []
    all_pred_coords = []
    
    for i in range(len(history_list)):
        history_frames = history_list[i]
        kf = kf_list[i]
        current_frame_id = current_frame_ids[i]
        pred_raw = pred_raw_all[i]
        
        future_frame_ids = [current_frame_id + 1 + t for t in range(3)]
        pred_results = []
        prev_w = history_frames[-1][2]
        prev_h = history_frames[-1][3]
        pred_coords = []
        
        for t in range(3):
            pred_cx, pred_cy = pred_raw[t]
            future_id = future_frame_ids[t]

            if kf is not None:
                kf.predict()
                pred_cx, pred_cy = kf.update(np.array([pred_cx, pred_cy]))

            # è¾¹ç•Œè£å‰ª
            pred_cx = np.clip(pred_cx, 0, CONFIG["img_w"])
            pred_cy = np.clip(pred_cy, 0, CONFIG["img_h"])
            x1 = pred_cx - prev_w / 2
            y1 = pred_cy - prev_h / 2
            x1 = np.clip(x1, 0, CONFIG["img_w"] - prev_w)
            y1 = np.clip(y1, 0, CONFIG["img_h"] - prev_h)

            pred_results.append({
                "frame_id": int(future_id),
                "x1": round(float(x1), 2),
                "y1": round(float(y1), 2),
                "w": round(float(prev_w), 2),
                "h": round(float(prev_h), 2),
                "conf": 0.90
            })
            pred_coords.append(np.array([x1 + prev_w/2, y1 + prev_h/2]))
        
        all_pred_results.append(pred_results)
        all_future_ids.append(future_frame_ids)
        all_pred_coords.append(pred_coords)
    
    return all_pred_results, all_future_ids, all_pred_coords

def continuous_infer_core(
        frame_data: List[FrameData],
        use_kalman: bool = False,
        conf_thresh: float = 0.5
) -> Dict[str, Any]:
    """æ ¸å¿ƒæ¨ç†å‡½æ•°"""
    start_time = time.time()
    try:
        real_frames, real_frame_ids = process_frame_data(frame_data, conf_thresh)

        history_frames = real_frames[:10].copy()
        history_frame_ids = real_frame_ids[:10].copy()
        all_predictions = []
        pred_records = []
        total_frames = len(real_frame_ids) - 10
        processed_count = 0

        # è·å–å¡å°”æ›¼æ»¤æ³¢å™¨
        kf = get_kf_from_pool() if use_kalman else None
        if kf and use_kalman:
            kf.init_state(history_frames[-1][:2])

        # å•è¯·æ±‚æ¨ç†
        while len(history_frames) == 10:
            current_frame_id = history_frame_ids[-1]
            processed_count += 1

            # å•æ ·æœ¬é¢„æµ‹
            pred_results, future_ids, pred_coords = predict_future_batch(
                history_list=[history_frames],
                kf_list=[kf],
                current_frame_ids=[current_frame_id]
            )
            pred_results = pred_results[0]
            future_ids = future_ids[0]
            pred_coords = pred_coords[0]

            all_predictions.append({
                "current_frame_id": int(current_frame_id),
                "future_frames": pred_results
            })

            # è®°å½•é¢„æµ‹ç»“æœ
            for t in range(3):
                future_id = future_ids[t]
                pred_cx, pred_cy = pred_coords[t]
                real_cx, real_cy = None, None
                
                if future_id in real_frame_ids:
                    real_idx = real_frame_ids.index(future_id)
                    real_cx, real_cy = real_frames[real_idx][0], real_frames[real_idx][1]
                
                pred_records.append({
                    "current_frame": int(current_frame_id),
                    "future_frame": int(future_id),
                    "pred_cx": float(pred_cx),
                    "pred_cy": float(pred_cy),
                    "real_cx": float(real_cx) if real_cx else None,
                    "real_cy": float(real_cy) if real_cy else None
                })

            # æ»‘åŠ¨çª—å£
            history_frames.pop(0)
            history_frame_ids.pop(0)
            first_future_id = future_ids[0]
            
            if first_future_id in real_frame_ids:
                real_idx = real_frame_ids.index(first_future_id)
                history_frames.append(real_frames[real_idx])
                history_frame_ids.append(first_future_id)
            else:
                break

        # å½’è¿˜å¡å°”æ›¼æ»¤æ³¢å™¨
        if kf:
            return_kf_to_pool(kf)

        # è®¡ç®—å‡†ç¡®ç‡
        valid_errors = []
        for record in pred_records:
            if record['real_cx'] and record['real_cy']:
                error = np.sqrt((record['pred_cx'] - record['real_cx']) ** 2 + 
                                (record['pred_cy'] - record['real_cy']) ** 2)
                valid_errors.append(error)

        accuracy = {}
        if valid_errors:
            frame1_errors = valid_errors[::3] if len(valid_errors) >= 3 else []
            frame2_errors = valid_errors[1::3] if len(valid_errors) >= 3 else []
            frame3_errors = valid_errors[2::3] if len(valid_errors) >= 3 else []

            accuracy = {
                "æ€»å¹³å‡è¯¯å·®(px)": round(float(np.mean(valid_errors)), 2),
                "æœªæ¥1å¸§å¹³å‡è¯¯å·®(px)": round(float(np.mean(frame1_errors)), 2) if frame1_errors else 0.0,
                "æœªæ¥2å¸§å¹³å‡è¯¯å·®(px)": round(float(np.mean(frame2_errors)), 2) if frame2_errors else 0.0,
                "æœªæ¥3å¸§å¹³å‡è¯¯å·®(px)": round(float(np.mean(frame3_errors)), 2) if frame3_errors else 0.0,
                "æœ€å¤§è¯¯å·®(px)": round(float(np.max(valid_errors)), 2),
                "æœ€å°è¯¯å·®(px)": round(float(np.min(valid_errors)), 2),
                "â‰¤5pxæˆåŠŸç‡(%)": round(float(sum(1 for e in valid_errors if e <= 5) / len(valid_errors) * 100), 2),
                "â‰¤10pxæˆåŠŸç‡(%)": round(float(sum(1 for e in valid_errors if e <= 10) / len(valid_errors) * 100), 2),
                "æœ‰æ•ˆé¢„æµ‹å¸§æ•°": len(valid_errors)
            }
        else:
            accuracy = {"æç¤º": "æ— æœ‰æ•ˆçœŸå®å€¼"}

        latency = time.time() - start_time
        update_metrics("success")
        
        return {
            "all_predictions": numpy_to_python(all_predictions),
            "accuracy": numpy_to_python(accuracy),
            "processed_frames": processed_count,
            "total_frames": total_frames,
            "device": str(DEVICE),
            "latency": round(latency, 4)
        }
    
    except Exception as e:
        update_metrics("error")
        raise e

# ===================== APIæ¥å£ =====================
@app.post("/predict", response_model=PredictResponse, summary="è½¨è¿¹é¢„æµ‹æ¥å£ï¼ˆé«˜å¹¶å‘ç‰ˆï¼‰")
@limiter.limit(CONFIG["rate_limit"])
async def predict_trajectory(
    request: Request,
    predict_req: PredictRequest,
    background_tasks: BackgroundTasks
):
    """å¼‚æ­¥æ¨ç†æ¥å£ï¼Œæ”¯æŒè¶…æ—¶æ§åˆ¶å’ŒGPUå¹¶å‘é™åˆ¶"""
    start_time = time.time()
    try:
        # GPUå¹¶å‘æ§åˆ¶
        if GPU_SEMAPHORE:
            async with GPU_SEMAPHORE:
                # æäº¤åˆ°çº¿ç¨‹æ± æ‰§è¡Œæ¨ç†
                result = await asyncio.wait_for(
                    asyncio.get_event_loop().run_in_executor(
                        EXECUTOR,
                        continuous_infer_core,
                        predict_req.frame_data,
                        predict_req.use_kalman,
                        predict_req.conf_thresh
                    ),
                    timeout=CONFIG["timeout"]
                )
        else:
            # CPUæ¨ç†
            result = await asyncio.wait_for(
                asyncio.get_event_loop().run_in_executor(
                    EXECUTOR,
                    continuous_infer_core,
                    predict_req.frame_data,
                    predict_req.use_kalman,
                    predict_req.conf_thresh
                ),
                timeout=CONFIG["timeout"]
            )
        
        latency = time.time() - start_time
        return PredictResponse(
            code=200,
            msg="é¢„æµ‹æˆåŠŸ",
            data={
                "é¢„æµ‹ç»“æœ": result["all_predictions"],
                "å‡†ç¡®ç‡ç»Ÿè®¡": result["accuracy"],
                "å¤„ç†å¸§æ•°": result["processed_frames"],
                "æ€»å¸§æ•°": result["total_frames"],
                "ä½¿ç”¨è®¾å¤‡": result["device"],
                "æ¨ç†è€—æ—¶(ç§’)": result["latency"],
                "è¯·æ±‚æ€»è€—æ—¶(ç§’)": round(latency, 4)
            },
            latency=round(latency, 4)
        )
    
    except asyncio.TimeoutError:
        update_metrics("timeout")
        logging.error(f"è¯·æ±‚è¶…æ—¶ï¼ˆ{CONFIG['timeout']}ç§’ï¼‰")
        return PredictResponse(
            code=500,
            msg="è¯·æ±‚è¶…æ—¶",
            error=f"è¯·æ±‚å¤„ç†è¶…æ—¶ï¼ˆè¶…è¿‡{CONFIG['timeout']}ç§’ï¼‰",
            latency=round(time.time() - start_time, 4)
        )
    except Exception as e:
        error_msg = str(e)
        logging.error(f"é¢„æµ‹å¤±è´¥ï¼š{error_msg}", exc_info=True)
        return PredictResponse(
            code=500,
            msg="é¢„æµ‹å¤±è´¥",
            error=error_msg,
            latency=round(time.time() - start_time, 4)
        )

@app.get("/health", summary="å¥åº·æ£€æŸ¥ï¼ˆé«˜å¹¶å‘ç‰ˆï¼‰")
async def health_check():
    """å¢å¼ºå‹å¥åº·æ£€æŸ¥"""
    gpu_mem = None
    if isinstance(DEVICE, torch.device) and DEVICE.type == "cuda":
        gpu_mem = {
            "å·²ç”¨(MB)": round(torch.cuda.memory_allocated() / 1024 / 1024, 2),
            "æœ€å¤§åˆ†é…(MB)": round(torch.cuda.max_memory_allocated() / 1024 / 1024, 2),
            "ç¼“å­˜(MB)": round(torch.cuda.memory_reserved() / 1024 / 1024, 2)
        }
    
    with METRICS_LOCK:
        metrics = dict(REQUEST_METRICS)
    
    return {
        "code": 200,
        "msg": "æœåŠ¡æ­£å¸¸ï¼ˆé«˜å¹¶å‘æ¨¡å¼ï¼‰",
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "cudaå¯ç”¨": torch.cuda.is_available(),
        "DEVICEç±»å‹": str(type(DEVICE)),
        "DEVICEå€¼": str(DEVICE),
        "æ¨¡å‹å·²åŠ è½½": MODEL is not None,
        "å¡å°”æ›¼æ»¤æ³¢å™¨æ± å¤§å°": len(KF_POOL),
        "çº¿ç¨‹æ± å¤§å°": CONFIG["max_workers"],
        "GPUæœ€å¤§å¹¶å‘": CONFIG["max_concurrent_gpu"],
        "å½“å‰é™æµ": CONFIG["rate_limit"],
        "GPUå†…å­˜ä½¿ç”¨": gpu_mem,
        "è¯·æ±‚ç»Ÿè®¡": {
            "æˆåŠŸæ•°": metrics.get("success", 0),
            "é”™è¯¯æ•°": metrics.get("error", 0),
            "è¶…æ—¶æ•°": metrics.get("timeout", 0),
            "æ€»è¯·æ±‚æ•°": metrics.get("success", 0) + metrics.get("error", 0) + metrics.get("timeout", 0)
        }
    }

@app.get("/metrics", summary="æ€§èƒ½æŒ‡æ ‡ç›‘æ§")
async def get_metrics():
    """è·å–å®æ—¶æ€§èƒ½æŒ‡æ ‡"""
    with METRICS_LOCK:
        metrics = dict(REQUEST_METRICS)
    
    return {
        "code": 200,
        "è¯·æ±‚ç»Ÿè®¡": metrics,
        "å¡å°”æ›¼æ»¤æ³¢å™¨æ± ä½¿ç”¨ç‡": 1 - len(KF_POOL)/CONFIG["kf_pool_size"],
        "GPUå¹¶å‘æ•°": CONFIG["max_concurrent_gpu"] - GPU_SEMAPHORE._value if GPU_SEMAPHORE else 0,
        "çº¿ç¨‹æ± å¤§å°": CONFIG["max_workers"],
        "æ‰¹å¤„ç†å¤§å°": CONFIG["batch_size"]
    }

# ===================== å¯åŠ¨æœåŠ¡ =====================
if __name__ == "__main__":
    # æœ€ç»ˆæ ¡éªŒDEVICEç±»å‹
    if not isinstance(DEVICE, torch.device):
        logging.critical(f"ğŸ’¥ DEVICE ä¸æ˜¯ torch.device å¯¹è±¡ï¼š{type(DEVICE)} â†’ {DEVICE}")
        exit(1)
    
    # Windowså…¼å®¹å¯åŠ¨
    uvicorn.run(
        app="__main__:app",
        host="0.0.0.0",
        port=8000,
        reload=False,
        loop="asyncio",
        access_log=True,
        log_level="info",
        timeout_keep_alive=5,
    )