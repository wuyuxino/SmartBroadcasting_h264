import os
import torch
import numpy as np
from flask import Flask, request, jsonify
import json
from typing import List, Dict, Optional
from datetime import datetime

app = Flask(__name__)

# å…¨å±€å˜é‡
device: torch.device = None
model = None
norm_stats: Dict = None

# ===================== æ¨¡å‹å®šä¹‰ =====================
class KFDeepLearningModel(torch.nn.Module):
    def __init__(self):
        super(KFDeepLearningModel, self).__init__()
        self.Q_log = torch.nn.Parameter(torch.log(torch.eye(4, dtype=torch.float32) * 0.1))
        self.R_log = torch.nn.Parameter(torch.log(torch.eye(2, dtype=torch.float32) * 1.0))

        self.F = torch.tensor([[1, 0, 1, 0],
                               [0, 1, 0, 1],
                               [0, 0, 1, 0],
                               [0, 0, 0, 1]], dtype=torch.float32)
        self.H = torch.tensor([[1, 0, 0, 0],
                               [0, 1, 0, 0]], dtype=torch.float32)
        self.init_P = torch.eye(4, dtype=torch.float32) * 1000.0

    @property
    def Q(self):
        return torch.exp(self.Q_log) + 1e-6 * torch.eye(4, dtype=torch.float32).to(self.Q_log.device)

    @property
    def R(self):
        return torch.exp(self.R_log) + 1e-6 * torch.eye(2, dtype=torch.float32).to(self.R_log.device)

    def forward(self, history_obs: torch.Tensor, norm_stats: dict = None, denorm: bool = True) -> torch.Tensor:
        F = self.F.to(history_obs.device)
        H = self.H.to(history_obs.device)
        init_P = self.init_P.to(history_obs.device)

        x0, y0 = history_obs[0, 0], history_obs[0, 1]
        X = torch.tensor([x0, y0, 0.0, 0.0], dtype=torch.float32).to(history_obs.device).reshape(4, 1)
        P = init_P.clone()

        for obs in history_obs:
            X = F @ X
            P = F @ P @ F.T + self.Q
            z = obs.reshape(2, 1)
            S = H @ P @ H.T + self.R
            K = P @ H.T @ torch.inverse(S)
            X = X + K @ (z - H @ X)
            P = (torch.eye(4).to(history_obs.device) - K @ H) @ P

        current_x, current_y = X[0, 0], X[1, 0]
        vx, vy = X[2, 0], X[3, 0]
        future_pred = []
        for k in range(1, 4):
            pred_x = current_x + k * vx
            pred_y = current_y + k * vy
            pred_tensor = torch.cat([pred_x.unsqueeze(0), pred_y.unsqueeze(0)])
            future_pred.append(pred_tensor)
        pred_norm = torch.stack(future_pred)

        if denorm and norm_stats is not None:
            pred = self.denormalize_coords(pred_norm, norm_stats)
            return pred
        return pred_norm
    
    def denormalize_coords(self, coords_norm: torch.Tensor, stats: dict) -> torch.Tensor:
        mean_x = torch.tensor(stats["mean_x"], dtype=coords_norm.dtype).to(coords_norm.device)
        mean_y = torch.tensor(stats["mean_y"], dtype=coords_norm.dtype).to(coords_norm.device)
        std_x = torch.tensor(stats["std_x"], dtype=coords_norm.dtype).to(coords_norm.device)
        std_y = torch.tensor(stats["std_y"], dtype=coords_norm.dtype).to(coords_norm.device)

        coords = coords_norm.clone()
        coords[:, 0] = coords[:, 0] * std_x + mean_x
        coords[:, 1] = coords[:, 1] * std_y + mean_y
        return coords

# ===================== è¾…åŠ©å‡½æ•° =====================
def normalize_coords(coords: torch.Tensor, stats: dict) -> torch.Tensor:
    """å½’ä¸€åŒ–åæ ‡"""
    mean_x = torch.tensor(stats["mean_x"], dtype=coords.dtype).to(coords.device)
    mean_y = torch.tensor(stats["mean_y"], dtype=coords.dtype).to(coords.device)
    std_x = torch.tensor(stats["std_x"], dtype=coords.dtype).to(coords.device)
    std_y = torch.tensor(stats["std_y"], dtype=coords.dtype).to(coords.device)

    coords_norm = coords.clone()
    coords_norm[:, 0] = (coords_norm[:, 0] - mean_x) / std_x
    coords_norm[:, 1] = (coords_norm[:, 1] - mean_y) / std_y
    return coords_norm

def load_norm_stats(stats_path: str) -> dict:
    """åŠ è½½å½’ä¸€åŒ–ç»Ÿè®¡é‡"""
    if not os.path.exists(stats_path):
        raise FileNotFoundError(f"å½’ä¸€åŒ–ç»Ÿè®¡é‡æ–‡ä»¶ä¸å­˜åœ¨ï¼š{stats_path}")
    with open(stats_path, "r") as f:
        stats = json.load(f)
    return stats

# ===================== æ¨¡å‹åŠ è½½ =====================
def load_model_service(model_path: str, stats_path: str):
    """æœåŠ¡å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹å’Œç»Ÿè®¡é‡"""
    global device, model, norm_stats
    
    try:
        # è®¾ç½®è®¾å¤‡
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"âœ… ä½¿ç”¨è®¾å¤‡: {device}")
        
        # åŠ è½½å½’ä¸€åŒ–ç»Ÿè®¡é‡
        norm_stats = load_norm_stats(stats_path)
        print(f"ğŸ“Š åŠ è½½å½’ä¸€åŒ–ç»Ÿè®¡é‡: mean_x={norm_stats['mean_x']:.2f}, mean_y={norm_stats['mean_y']:.2f}")
        
        # åŠ è½½æ¨¡å‹
        model = KFDeepLearningModel()
        model = model.to(device)
        
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼š{model_path}")
        
        checkpoint = torch.load(model_path, map_location=device, weights_only=True)
        model.load_state_dict(checkpoint["model_state_dict"])
        model.eval()
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ | è®­ç»ƒæœ€ä¼˜éªŒè¯æŸå¤±ï¼š{checkpoint.get('best_val_loss', 'N/A'):.6f}")
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{str(e)}")
        return False

# ===================== æ•°æ®å¤„ç† =====================
def validate_frame_data(frame_data_list: List[Dict]) -> bool:
    """éªŒè¯è¾“å…¥æ•°æ®æ ¼å¼"""
    required_fields = ['frame_id', 'x', 'y']
    
    for frame_dict in frame_data_list:
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        for field in required_fields:
            if field not in frame_dict:
                return False, f"ç¼ºå°‘å­—æ®µ: {field}"
        
        # æ£€æŸ¥æ•°æ®ç±»å‹
        try:
            int(frame_dict['frame_id'])
            float(frame_dict['x'])
            float(frame_dict['y'])
        except ValueError:
            return False, "å­—æ®µç±»å‹é”™è¯¯: frame_idåº”ä¸ºæ•´æ•°, x,yåº”ä¸ºæµ®ç‚¹æ•°"
    
    return True, ""

def preprocess_history(frame_data_list: List[Dict]) -> torch.Tensor:
    """é¢„å¤„ç†å†å²æ•°æ®ï¼Œæå–5å¸§åæ ‡"""
    global norm_stats
    
    # æŒ‰å¸§å·æ’åº
    sorted_frames = sorted(frame_data_list, key=lambda x: x['frame_id'])
    
    # æå–æœ€å5å¸§ï¼ˆæˆ–å…¨éƒ¨å¦‚æœä¸è¶³5å¸§ï¼‰
    if len(sorted_frames) >= 5:
        history_frames = sorted_frames[-5:]
    else:
        print(f"âš ï¸  å†å²å¸§æ•°ä¸è¶³5å¸§ï¼ˆå®é™…{len(sorted_frames)}å¸§ï¼‰ï¼Œä½¿ç”¨æ‰€æœ‰å¯ç”¨å¸§")
        history_frames = sorted_frames
    
    # æå–åæ ‡
    coords = []
    for frame in history_frames:
        coords.append([frame['x'], frame['y']])
    
    # è½¬æ¢ä¸ºtensorå¹¶å½’ä¸€åŒ–
    coords_tensor = torch.tensor(coords, dtype=torch.float32)
    coords_norm = normalize_coords(coords_tensor, norm_stats)
    
    return coords_norm, history_frames

# ===================== é¢„æµ‹æ¥å£ =====================
@app.route('/predict', methods=['POST'])
def predict():
    """é¢„æµ‹æ¥å£
    è¯·æ±‚ä½“æ ¼å¼:
    {
        "frame_data": [
            {
                "frame_id": int,      # å¸§å·
                "x": float,           # xåæ ‡
                "y": float            # yåæ ‡
            },
            ...
        ],
        "seg_name": str               # å¯é€‰çš„ç‰‡æ®µåç§°ï¼ˆç”¨äºè®°å½•ï¼‰
    }
    """
    global model, device, norm_stats
    
    try:
        # è§£æè¯·æ±‚æ•°æ®
        data = request.json
        if not data or "frame_data" not in data:
            return jsonify({"error": "ç¼ºå°‘frame_dataå­—æ®µ"}), 400
        
        frame_data_list = data["frame_data"]
        seg_name = data.get("seg_name", "unknown")
        
        # éªŒè¯æ•°æ®æ ¼å¼
        is_valid, error_msg = validate_frame_data(frame_data_list)
        if not is_valid:
            return jsonify({"error": f"æ•°æ®æ ¼å¼é”™è¯¯: {error_msg}"}), 400
        
        # æ£€æŸ¥æ•°æ®é‡
        if len(frame_data_list) < 1:
            return jsonify({"error": "frame_dataä¸èƒ½ä¸ºç©º"}), 400
        
        # é¢„å¤„ç†å†å²æ•°æ®
        try:
            history_norm, history_frames = preprocess_history(frame_data_list)
            history_norm = history_norm.to(device)
        except Exception as e:
            return jsonify({"error": f"æ•°æ®é¢„å¤„ç†å¤±è´¥: {str(e)}"}), 400
        
        # è·å–å½“å‰æœ€å¤§å¸§å·
        current_frame_id = max(frame['frame_id'] for frame in frame_data_list)
        
        # æ¨¡å‹é¢„æµ‹
        with torch.no_grad():
            future_pred = model(history_norm, norm_stats=norm_stats, denorm=True)
            future_pred_np = future_pred.cpu().numpy()
        
        # ç”Ÿæˆæœªæ¥å¸§å·ï¼ˆä»å½“å‰å¸§+1å¼€å§‹ï¼‰
        future_frame_ids = [current_frame_id + 1 + i for i in range(3)]
        
        # å‡†å¤‡å†å²æ•°æ®ä¿¡æ¯
        history_info = []
        for i, frame in enumerate(history_frames):
            history_info.append({
                "frame_id": frame['frame_id'],
                "x": round(float(frame['x']), 2),
                "y": round(float(frame['y']), 2)
            })
        
        # å‡†å¤‡é¢„æµ‹ç»“æœ
        predictions = []
        for i in range(3):
            predictions.append({
                "future_frame_id": future_frame_ids[i],
                "x": round(float(future_pred_np[i, 0]), 2),
                "y": round(float(future_pred_np[i, 1]), 2)
            })
        
        # æ„å»ºå“åº”
        response = {
            "status": "success",
            "seg_name": seg_name,
            "current_frame_id": current_frame_id,
            "history_frames_used": len(history_frames),
            "history": history_info,
            "predictions": predictions,
            "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "model_info": {
                "history_frames": 5,
                "future_frames": 3,
                "device": str(device)
            }
        }
        
        return jsonify(response)
        
    except Exception as e:
        print(f"âŒ é¢„æµ‹å¤±è´¥: {str(e)}")
        return jsonify({"error": f"é¢„æµ‹å¤±è´¥: {str(e)}"}), 500

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    global model, norm_stats
    
    if model is None or norm_stats is None:
        return jsonify({"status": "unhealthy", "message": "æ¨¡å‹æœªåŠ è½½"}), 503
    
    return jsonify({
        "status": "healthy",
        "model_loaded": model is not None,
        "stats_loaded": norm_stats is not None,
        "device": str(device),
        "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    })

@app.route('/model_info', methods=['GET'])
def model_info():
    """è·å–æ¨¡å‹ä¿¡æ¯"""
    global model, norm_stats
    
    if model is None or norm_stats is None:
        return jsonify({"error": "æ¨¡å‹æœªåŠ è½½"}), 503
    
    info = {
        "model_type": "KFDeepLearningModel",
        "input_frames": 5,
        "output_frames": 3,
        "normalization_stats": {
            "mean_x": norm_stats.get("mean_x"),
            "mean_y": norm_stats.get("mean_y"),
            "std_x": norm_stats.get("std_x"),
            "std_y": norm_stats.get("std_y")
        },
        "device": str(device)
    }
    
    return jsonify(info)

# ===================== æœåŠ¡å¯åŠ¨ =====================
if __name__ == "__main__":
    # é…ç½®æ–‡ä»¶è·¯å¾„ - è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹
    CONFIG = {
        "model_path": "./trained_kf_model.pth",      # ä½ çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„
        "stats_path": "./norm_stats.json",           # å½’ä¸€åŒ–ç»Ÿè®¡é‡æ–‡ä»¶è·¯å¾„
        "host": "0.0.0.0",
        "port": 8000,                                # æœåŠ¡ç«¯å£
        "debug": False
    }
    
    # åŠ è½½æ¨¡å‹
    print("=" * 50)
    print("ğŸš€ å¼€å§‹åŠ è½½æ¨¡å‹...")
    load_success = load_model_service(CONFIG["model_path"], CONFIG["stats_path"])
    
    if not load_success:
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼ŒæœåŠ¡é€€å‡º")
        exit(1)
    
    print("=" * 50)
    print(f"âœ… æ¨¡å‹æœåŠ¡å‡†å¤‡å°±ç»ª")
    print(f"ğŸ“¡ æœåŠ¡åœ°å€: http://{CONFIG['host']}:{CONFIG['port']}")
    print(f"ğŸ“Œ é¢„æµ‹æ¥å£: POST http://{CONFIG['host']}:{CONFIG['port']}/predict")
    print(f"ğŸ“Œ å¥åº·æ£€æŸ¥: GET  http://{CONFIG['host']}:{CONFIG['port']}/health")
    print(f"ğŸ“Œ æ¨¡å‹ä¿¡æ¯: GET  http://{CONFIG['host']}:{CONFIG['port']}/model_info")
    print("=" * 50)
    
    # å¯åŠ¨FlaskæœåŠ¡
    app.run(
        host=CONFIG["host"],
        port=CONFIG["port"],
        debug=CONFIG["debug"],
        threaded=True
    )